{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentimentdataset.csv\")\n",
    "\n",
    "# Clean text\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()                                # lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # remove extra spaces\n",
    "    words = [w for w in text.split() if w not in stop_words]  # remove stopwords\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0575b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT MAPPING - Apply before any analysis\n",
    "df['Sentiment_clean'] = df['Sentiment'].str.lower().str.strip()\n",
    "\n",
    "# Define sentiment groups\n",
    "negative = [\n",
    "    'negative', 'anger', 'fear', 'sadness', 'disgust', 'disappointed', 'bitter',\n",
    "    'shame', 'despair', 'grief', 'loneliness', 'jealousy', 'resentment', 'frustration',\n",
    "    'anxiety', 'helplessness', 'envy', 'regret', 'bitterness',\n",
    "    'sorrow', 'loss', 'darkness', 'depression', 'fearful', 'heartbreak',\n",
    "    'apprehensive', 'overwhelmed'\n",
    "]\n",
    "\n",
    "neutral = [\n",
    "    'neutral', 'acceptance', 'calmness', 'indifference', 'contemplation',\n",
    "    'ambivalence', 'reflection', 'solitude', 'melancholy', 'nostalgia',   'pensive'\n",
    "]\n",
    "\n",
    "positive = [\n",
    "     'amusement', 'enjoyment', 'admiration', 'joy', 'love', 'pride', 'confidence',\n",
    "    'affection', 'awe', 'hope', 'gratitude', 'contentment', 'serenity',\n",
    "    'enthusiasm', 'fulfillment', 'positive', 'happiness',  'optimism', 'playful',\n",
    "    'curiosity', 'excitement', 'kindness', 'friendship', 'success',\n",
    "    'thrill', 'surprise', 'cheerfulness', 'inspiration', 'freedom', 'blessed', 'satisfaction'\n",
    "]\n",
    "\n",
    "# Create mapping function\n",
    "def map_sentiment(s):\n",
    "    s = s.lower().strip()\n",
    "    if s in positive:\n",
    "        return 2  # positive\n",
    "    elif s in neutral:\n",
    "        return 1  # neutral\n",
    "    elif s in negative:\n",
    "        return 0  # negative\n",
    "    else:\n",
    "        return -1  # unknown\n",
    "\n",
    "df['label'] = df['Sentiment_clean'].apply(map_sentiment)\n",
    "\n",
    "# Drop unknown labels\n",
    "df = df[df['label'] != -1]\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nLabel mapping: 0=Negative, 1=Neutral, 2=Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c52689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Clean_Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12097e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Additional Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602921d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_spacy(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(clean_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e22561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: POS Tagging & Distribution\n",
    "def get_pos_counts(text):\n",
    "    doc = nlp(text)\n",
    "    return Counter([token.pos_ for token in doc])\n",
    "\n",
    "df[\"POS_Counts\"] = df[\"Clean_Text\"].apply(get_pos_counts)\n",
    "\n",
    "# Aggregate by sentiment label\n",
    "sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "pos_by_sentiment = {}\n",
    "for label in sorted(df[\"label\"].unique()):\n",
    "    subset = df[df[\"label\"] == label]\n",
    "    combined = Counter()\n",
    "    for counts in subset[\"POS_Counts\"]:\n",
    "        combined.update(counts)\n",
    "    pos_by_sentiment[sentiment_names[label]] = combined\n",
    "\n",
    "# Plot POS distribution\n",
    "for sentiment_name, counts in pos_by_sentiment.items():\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(counts.keys(), counts.values())\n",
    "    plt.title(f\"POS Distribution for {sentiment_name} Sentiment\")\n",
    "    plt.xlabel(\"POS Tag\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec34dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Lemmatization\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n",
    "df[\"Lemma_Text\"] = df[\"Clean_Text\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Word Clouds for each sentiment\n",
    "sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "for label in sorted(df[\"label\"].unique()):\n",
    "    text = \" \".join(df[df[\"label\"] == label][\"Lemma_Text\"])\n",
    "    if text.strip():\n",
    "        wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(wc, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Word Cloud for {sentiment_names[label]} Sentiment\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Vectorization (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"Lemma_Text\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set distribution:\\n{pd.Series(y_train).value_counts().sort_index()}\")\n",
    "print(f\"\\nTest set distribution:\\n{pd.Series(y_test).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708101c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Model Training & Evaluation\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "print(\"=\" * 50)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa292bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy on Test Set:\")\n",
    "for idx, sentiment_name in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    mask = (y_test == idx)\n",
    "    if np.sum(mask) > 0:\n",
    "        class_acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "        print(f\"{sentiment_name}: {class_acc:.4f} ({np.sum(mask)} samples)\")\n",
    "    else:\n",
    "        print(f\"{sentiment_name}: No test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a239697e",
   "metadata": {},
   "source": [
    "## Normalized POS Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c18579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized POS distributions by label\n",
    "sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "normalized_pos_by_sentiment = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALIZED POS DISTRIBUTION (as percentage of total POS per label)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in sorted(df[\"label\"].unique()):\n",
    "    subset = df[df[\"label\"] == label]\n",
    "    combined = Counter()\n",
    "    \n",
    "    # Count all POS tags for this label\n",
    "    for counts in subset[\"POS_Counts\"]:\n",
    "        combined.update(counts)\n",
    "    \n",
    "    # Calculate total POS count for normalization\n",
    "    total_pos_count = sum(combined.values())\n",
    "    \n",
    "    # Normalize: divide each count by total\n",
    "    normalized = {pos: (count / total_pos_count * 100) for pos, count in combined.items()}\n",
    "    normalized_pos_by_sentiment[sentiment_names[label]] = normalized\n",
    "    \n",
    "    # Sort by percentage and print\n",
    "    sorted_pos = sorted(normalized.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\n{sentiment_names[label]} ({total_pos_count} total POS tags):\")\n",
    "    print(\"-\" * 60)\n",
    "    for pos, percentage in sorted_pos:\n",
    "        print(f\"  {pos:8s}: {percentage:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaaa6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized POS distributions as ordered histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (sentiment_name, normalized_dist) in enumerate(sorted(normalized_pos_by_sentiment.items())):\n",
    "    # Sort by percentage (descending)\n",
    "    sorted_dist = dict(sorted(normalized_dist.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    pos_tags = list(sorted_dist.keys())\n",
    "    percentages = list(sorted_dist.values())\n",
    "    \n",
    "    # Create bar chart\n",
    "    axes[idx].bar(pos_tags, percentages, color='steelblue')\n",
    "    axes[idx].set_title(f'{sentiment_name} Sentiment\\n(Normalized POS Distribution)', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].set_xlabel('POS Tag')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].set_ylim(0, max(percentages) * 1.1)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative view: All POS tags side-by-side with normalized values\n",
    "all_pos_tags = set()\n",
    "for dist in normalized_pos_by_sentiment.values():\n",
    "    all_pos_tags.update(dist.keys())\n",
    "\n",
    "all_pos_tags = sorted(list(all_pos_tags))\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {}\n",
    "for sentiment_name in ['Negative', 'Neutral', 'Positive']:\n",
    "    comparison_data[sentiment_name] = [\n",
    "        normalized_pos_by_sentiment[sentiment_name].get(pos, 0) for pos in all_pos_tags\n",
    "    ]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data, index=all_pos_tags)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARATIVE NORMALIZED POS DISTRIBUTION (Percentage)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Plot grouped bar chart for easy comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(all_pos_tags))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, comparison_df['Negative'], width, label='Negative', color='#d62728')\n",
    "bars2 = ax.bar(x, comparison_df['Neutral'], width, label='Neutral', color='#ff7f0e')\n",
    "bars3 = ax.bar(x + width, comparison_df['Positive'], width, label='Positive', color='#2ca02c')\n",
    "\n",
    "ax.set_xlabel('POS Tag', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Normalized POS Distribution Comparison Across Sentiments', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_pos_tags, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Show differences between sentiments\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"POS DIFFERENCES BETWEEN SENTIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Which POS tags are most characteristic of each sentiment?\n",
    "for sentiment in ['Negative', 'Neutral', 'Positive']:\n",
    "    other_sentiments = [s for s in ['Negative', 'Neutral', 'Positive'] if s != sentiment]\n",
    "    \n",
    "    # Calculate average difference from other sentiments\n",
    "    differences = {}\n",
    "    for pos in all_pos_tags:\n",
    "        current_val = comparison_df.loc[pos, sentiment]\n",
    "        other_avg = comparison_df.loc[pos, other_sentiments].mean()\n",
    "        differences[pos] = current_val - other_avg\n",
    "    \n",
    "    # Sort and show top 5\n",
    "    sorted_diff = sorted(differences.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nMost characteristic POS tags for {sentiment}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for pos, diff in sorted_diff[:5]:\n",
    "        val = comparison_df.loc[pos, sentiment]\n",
    "        print(f\"  {pos:8s}: {val:6.2f}% (diff: +{diff:6.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c18a6",
   "metadata": {},
   "source": [
    "## Binary Classification: Positive vs Negative Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification dataset (only Positive and Negative)\n",
    "df_binary = df[df['label'] != 1].copy()  # Remove neutral (label 1)\n",
    "df_binary['label'] = df_binary['label'].map({0: 0, 2: 1})  # Map: Negative=0, Positive=1\n",
    "\n",
    "print(\"Binary Classification Dataset:\")\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Binary dataset size (Positive + Negative): {len(df_binary)}\")\n",
    "print(f\"\\nBinary label distribution:\\n{df_binary['label'].value_counts().sort_index()}\")\n",
    "print(f\"\\nLabel mapping: 0=Negative, 1=Positive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13195069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Vectorization (TF-IDF)\n",
    "vectorizer_binary = TfidfVectorizer(max_features=5000)\n",
    "X_binary = vectorizer_binary.fit_transform(df_binary[\"Lemma_Text\"])\n",
    "y_binary = df_binary[\"label\"]\n",
    "\n",
    "print(f\"Binary TF-IDF matrix shape: {X_binary.shape}\")\n",
    "print(f\"Number of features: {X_binary.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e460f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Train-Test Split\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(f\"Binary Training samples: {X_train_binary.shape[0]}\")\n",
    "print(f\"Binary Test samples: {X_test_binary.shape[0]}\")\n",
    "print(f\"\\nBinary Training set distribution:\\n{pd.Series(y_train_binary).value_counts().sort_index()}\")\n",
    "print(f\"\\nBinary Test set distribution:\\n{pd.Series(y_test_binary).value_counts().sort_index()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Model Training & Evaluation\n",
    "model_binary = MultinomialNB()\n",
    "model_binary.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "y_pred_binary = model_binary.predict(X_test_binary)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BINARY MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test_binary, y_pred_binary):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, y_pred_binary, zero_division=0):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test_binary, y_pred_binary, zero_division=0):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test_binary, y_pred_binary, zero_division=0):.4f}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_binary, target_names=['Negative', 'Positive'], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Confusion Matrix\n",
    "cm_binary = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_binary, annot=True, fmt='d', cmap='RdYlGn', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - Binary Classification (Test Set)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBinary Confusion Matrix:\")\n",
    "print(cm_binary)\n",
    "print(\"\\nBreakdown:\")\n",
    "print(f\"True Negatives (TN): {cm_binary[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm_binary[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm_binary[1, 0]}\")\n",
    "print(f\"True Positives (TP): {cm_binary[1, 1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf16096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Model Predictions with Confidence Scores\n",
    "y_pred_proba_binary = model_binary.predict_proba(X_test_binary)\n",
    "\n",
    "# Create predictions dataframe\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label': ['Negative' if y == 0 else 'Positive' for y in y_test_binary],\n",
    "    'Predicted_Label': ['Negative' if y == 0 else 'Positive' for y in y_pred_binary],\n",
    "    'Confidence_Negative': y_pred_proba_binary[:, 0],\n",
    "    'Confidence_Positive': y_pred_proba_binary[:, 1],\n",
    "    'Prediction_Correct': y_pred_binary == y_test_binary\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"BINARY MODEL PREDICTIONS (First 20 samples)\")\n",
    "print(\"=\" * 100)\n",
    "print(predictions_df.head(20).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total Correct: {predictions_df['Prediction_Correct'].sum()}/{len(predictions_df)}\")\n",
    "print(f\"Total Incorrect: {(~predictions_df['Prediction_Correct']).sum()}/{len(predictions_df)}\")\n",
    "print(f\"\\nAccuracy: {predictions_df['Prediction_Correct'].sum() / len(predictions_df):.4f}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(f\"\\nNegative Accuracy: {predictions_df[(predictions_df['True_Label'] == 'Negative') & (predictions_df['Prediction_Correct'])].shape[0] / (predictions_df['True_Label'] == 'Negative').sum():.4f}\")\n",
    "print(f\"Positive Accuracy: {predictions_df[(predictions_df['True_Label'] == 'Positive') & (predictions_df['Prediction_Correct'])].shape[0] / (predictions_df['True_Label'] == 'Positive').sum():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7937a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef65320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
